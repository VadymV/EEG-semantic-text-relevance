# Copyright 2024
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
This script generates the results of the benchmark experiments.
Run it with ``poetry run python generate_results.py --project_path=path``
The project_path should point to the files generated by ``benchmark.py``
The generated results are saved to a log_results.log file.
"""
import argparse
import glob
import logging
import os

import numpy as np
import pandas as pd
import seaborn as sns
import torch.nn
from matplotlib import pyplot as plt
from torch import tensor
from torchmetrics.classification import BinaryCohenKappa, BinaryPrecision, \
    BinaryRecall, BinaryMatthewsCorrCoef, BinaryAUROC

from src.releegance.misc.utils import set_logging, set_seed, create_args


def run(file_pattern: str, args: argparse.Namespace):
    # Read predictions:
    filepaths = glob.glob(
        os.path.join(args.project_path, file_pattern))
    if not filepaths:
        logging.warning('No files found. Quitting...')
        return
    if file_pattern.startswith("s"):
        level = "sentence"
    if file_pattern.startswith("w"):
        level = "word"
    results = []
    for fp in filepaths:
        with open(fp, 'r') as f:
            logging.info('Reading %s', fp)
            results.append(pd.read_pickle(f.name))
    results = pd.concat(results)

    # Apply sigmoid to predictions based on the model type:
    mask = (results['model'].isin(['eegnet', 'lstm', 'uercm']))
    masked_results = results.loc[mask]
    results.loc[mask, 'predictions'] = masked_results['predictions'].apply(
        lambda x: torch.nn.Sigmoid()(torch.FloatTensor([x])).tolist().pop())

    # Calculate classification metrics:
    groups = results.groupby(
        ['seed', 'user', 'model', 'strategy'])
    mcc = groups.apply(
        lambda x: BinaryMatthewsCorrCoef()(tensor(x.predictions.tolist()),
                                           tensor(x.targets.tolist())).item(),
        include_groups=False).reset_index()
    mcc.rename(columns={0: 'mcc'}, inplace=True)
    kappa = groups.apply(
        lambda x: BinaryCohenKappa()(tensor(x.predictions.tolist()),
                                     tensor(x.targets.tolist())).item(),
        include_groups=False).reset_index()
    kappa.rename(columns={0: 'kappa'}, inplace=True)

    precision = groups.apply(
        lambda x: BinaryPrecision()(tensor(x.predictions.tolist()),
                                    tensor(x.targets.tolist())).item(),
        include_groups=False).reset_index()
    precision.rename(columns={0: 'precision'}, inplace=True)

    recall = groups.apply(
        lambda x: BinaryRecall()(tensor(x.predictions.tolist()),
                                 tensor(x.targets.tolist())).item(),
        include_groups=False).reset_index()
    recall.rename(columns={0: 'recall'}, inplace=True)

    auc = groups.apply(
        lambda x: BinaryAUROC()(tensor(x.predictions.tolist()),
                                tensor(x.targets.tolist())).item(),
        include_groups=False).reset_index()
    auc.rename(columns={0: 'auc'}, inplace=True)

    # Concatenate metrics:
    metrics = mcc.merge(precision, on=['seed', 'user', 'model', 'strategy'])
    metrics = metrics.merge(kappa, on=['seed', 'user', 'model', 'strategy'])
    metrics = metrics.merge(recall, on=['seed', 'user', 'model', 'strategy'])
    metrics = metrics.merge(auc, on=['seed', 'user', 'model', 'strategy'])

    for model in metrics.model.unique():
        latex_output = ''
        for strategy in metrics.strategy.unique()[::-1]:
            logging.info('\n\nModel: %s, Strategy: %s', model, strategy)
            for metric in ['auc', 'precision', 'recall']:
                mean = metrics[(metrics['model'] == model) & (
                        metrics['strategy'] == strategy)][
                    metric].mean()
                std = metrics[(metrics['model'] == model) & (
                        metrics['strategy'] == strategy)][
                    metric].std()
                logging.info('%s: %.2f +- %.2f', metric, mean, std)
                latex_output += f'& {mean:.2f} ({std:.2f}) '

        logging.info(latex_output)


    strategy_dependent = np.empty((5, 15))
    strategy_independent = np.empty((5, 15))
    for participant_id, participant in enumerate(metrics['user'].unique()):
        d = metrics[(metrics['user'] == participant)]
        for model_id, model in enumerate(metrics.model.unique()):
            for strategy_id, strategy in enumerate(metrics.strategy.unique()[::-1]):
                for metric in ['auc', 'precision', 'recall']:
                    mean = d[(d['model'] == model) & (
                            d['strategy'] == strategy)][
                        metric].mean()

                    if metric == 'auc' and strategy == "participant-dependent":
                        strategy_dependent[model_id, participant_id] = round(mean, 2)

                    if metric == 'auc' and strategy == "participant-independent":
                        strategy_independent[model_id, participant_id] = round(mean, 2)

    cmap = sns.cubehelix_palette(light=1, gamma=.6, n_colors=50, rot=-0.4, as_cmap=True)
    sns.set(font_scale=2)
    sns.set_theme(style="white")

    model_dict = {'eegnet': 'EEGNet', 'lda': 'LDA', 'lr': 'LR',
                  'lstm': 'LSTM', 'uercm': 'UERCM'}

    model_names = []
    for model_ in metrics['model'].unique():
        model_names.append(model_dict[model_])

    fig, axes = plt.subplots(2, 1, sharex=True,
                             gridspec_kw={'hspace': 0.02, 'wspace': 0.02})

    # Plot within-subject
    axes[0].imshow(strategy_dependent, cmap=cmap)
    for (i, j), z in np.ndenumerate(strategy_dependent):
        axes[0].text(j, i, f'{z}', ha='center', va='center', size=10)
    axes[0].set_yticks(np.arange(len(model_names)))
    axes[0].set_yticklabels(model_names)
    axes[0].annotate("Within-subject", xy=(1.02, 0.5), xycoords='axes fraction', fontsize=14,
                ha='left', va='center', rotation=90)

    # Plot cross-subject
    axes[1].imshow(strategy_independent, cmap=cmap)
    for (i, j), z in np.ndenumerate(strategy_independent):
        axes[1].text(j, i, f'{z}', ha='center', va='center', size=10)
    axes[1].set_yticks(np.arange(len(model_names)))
    axes[1].set_yticklabels(model_names)
    axes[1].annotate("Cross-subject", xy=(1.02, 0.5), xycoords='axes fraction', fontsize=14,
                ha='left', va='center', rotation=90)
    axes[1].annotate("Model", xy=(-0.2, 1.0), xycoords='axes fraction',
                     fontsize=14,
                     ha='left', va='center', rotation=90)

    plt.xticks(np.arange(len(metrics['user'].unique())),
               metrics['user'].unique())
    plt.xticks(rotation=90)
    plt.xlabel('Participant', fontsize=14, labelpad=10)

    plt.savefig(f"figures/results_participants_{level}.pdf",
                format="pdf", bbox_inches="tight")


if __name__ == '__main__':
    parser = create_args(seeds_args=False, benchmark_args=False)
    args = parser.parse_args()

    set_logging(args.project_path, file_name='logs_results')
    set_seed(1)
    logging.info('Args: %s', args)

    logging.info('Generating results for word relevance...')
    run(file_pattern='w_relevance_seed*.pkl', args=args)

    logging.info('Generating results for sentence relevance...')
    run(file_pattern='s_relevance_seed*.pkl', args=args)
